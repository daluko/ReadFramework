In recent years, the demand for tools to be able to
recognize, search and retrieve written and spoken sources
of multilingual information has increased tremendously.
The involvement of multinational groups in activities all
over the world, the proliferation of information via the
Internet and the general explosion of information
available in multiple languages have made cross
language communications essential to many
organizations. Although a lot of information that would
be useful for training cross language systems is available
online, there are still significant resources that can only
be accessed in printed form. In order to parse these
documents correctly, however, we must be able to
identify various scripts before choosing an appropriate
OCR algorithm. The identification of multiple scripts is
also required for many other types of documents such as
patents [9], and in multilingual document retrieval [7]. In
the case of dictionaries, the ability to identify font-style
(normal, bold, italics, all caps, underline) and the ability
to recognize special iconic symbols is also essential to
parsing the document’s functional structure.
For more general document analysis applications, font,
font-face and font-style recognition, are fundamental
issues. The accuracy of font-face recognition has a
significant impact on the performance of OCR. For
multilingual documents, font recognition is compounded
by the need to identify the script, and variation in the
font-style (bold, italic, underline and normal) can
significantly affect the performance of an OCR system,
so a priori classification can help improve the
performance.
2. Related work
In work on script identification, Hochberg et al. [2]
described a technique for identifying 13 scripts including
highly connected ones. In their algorithm, a scalenormalized
cluster template is created for each script
based on frequent characters or word shapes of this script,
then scripts are classified by comparing a subset of the
document’s textual symbols with these templates. In
[9][10], Spitz et al. initially divided scripts into Asian
scripts (Chinese, Japanese and Korean) and Roman based
on the observation that upward concavities are
distributed evenly along the vertical axis of Asian
characters, but they tend to appear also at certain
locations in Roman characters. Further distinctions
among Asian scripts are made on the basis of character
density. Waked et al. [1] used features of the horizontal
projection of a text line to classify scripts into three
categories including Arabic, Roman and Ideographic.
For font related classification, Manna et al. [4] used
the tangent distance as a classification function in a
nearest neighbor approach, and a TD-Neuron
discriminant model was employed to discriminate
between two similar classes. Kavallieratou et al. [6]
presented a slant removal algorithm based on the vertical
projection profile of a word image and the Wigner-Ville
distribution. This approach can be used to identify the
italic font-style of words. Vinciarelli et al. [8] presented
an algorithm which detects the slant based on the number